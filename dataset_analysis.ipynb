{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdad5c12",
   "metadata": {},
   "source": [
    "# GPU Cluster Spot Resource Dataset Analysis (Detailed Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50757437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTDIR = Path(\"outputs\")\n",
    "FIGDIR = OUTDIR / \"figures\"\n",
    "TBLDIR = OUTDIR / \"tables\"\n",
    "OUTDIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGDIR.mkdir(exist_ok=True, parents=True)\n",
    "TBLDIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "# node_df = pd.read_csv(\"node_info_df.csv\")\n",
    "# job_df = pd.read_csv(\"job_info_df.csv\")\n",
    "\n",
    "node_df = pd.read_csv('/kaggle/input/alibaba-gpu-cluster-spot-resource-dataset/node_info_df.csv')\n",
    "job_df = pd.read_csv('/kaggle/input/alibaba-gpu-cluster-spot-resource-dataset/job_info_df.csv')\n",
    "\n",
    "def human(n):\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\", \"T\"]:\n",
    "        if abs(n) < 1000.0:\n",
    "            return f\"{n:,.2f}{unit}\"\n",
    "        n /= 1000.0\n",
    "    return f\"{n:,.2f}P\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0223943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes rows and columns: (4278, 4)\n",
      "Jobs rows and columns: (466867, 9)\n",
      "Duplicate job_name count: 0\n",
      "Enriching jobs...\n"
     ]
    }
   ],
   "source": [
    "# Basic structure\n",
    "print(\"Nodes rows and columns:\", node_df.shape)\n",
    "print(\"Jobs rows and columns:\", job_df.shape)\n",
    "\n",
    "# Data quality checks\n",
    "node_nulls = node_df.isna().sum().sort_values(ascending=False)\n",
    "job_nulls = job_df.isna().sum().sort_values(ascending=False)\n",
    "node_nulls.to_csv(TBLDIR / \"node_null_counts.csv\", header=[\"nulls\"])\n",
    "job_nulls.to_csv(TBLDIR / \"job_null_counts.csv\", header=[\"nulls\"])\n",
    "\n",
    "dupe_jobs = job_df.duplicated(subset=[\"job_name\"]).sum() if \"job_name\" in job_df.columns else np.nan\n",
    "print(\"Duplicate job_name count:\", dupe_jobs)\n",
    "\n",
    "# Enrich jobs\n",
    "print(\"Enriching jobs...\")\n",
    "if \"gpu_request\" not in job_df.columns or \"worker_num\" not in job_df.columns:\n",
    "    raise ValueError(\"gpu_request and worker_num must exist in job_info_df.csv\")\n",
    "\n",
    "if \"cpu_request\" not in job_df.columns:\n",
    "    raise ValueError(\"cpu_request must exist in job_info_df.csv\")\n",
    "\n",
    "if \"duration\" not in job_df.columns:\n",
    "    raise ValueError(\"duration must exist in job_info_df.csv\")\n",
    "\n",
    "if \"submit_time\" not in job_df.columns:\n",
    "    raise ValueError(\"submit_time must exist in job_info_df.csv\")\n",
    "\n",
    "job_df = job_df.copy()\n",
    "job_df[\"gpu_demand\"] = job_df[\"gpu_request\"] * job_df[\"worker_num\"]\n",
    "job_df[\"gpu_seconds\"] = job_df[\"gpu_demand\"] * job_df[\"duration\"]\n",
    "job_df[\"cpu_seconds\"] = job_df[\"cpu_request\"] * job_df[\"duration\"]\n",
    "job_df[\"submit_day\"] = (job_df[\"submit_time\"] // 86400).astype(int)\n",
    "job_df[\"submit_hour\"] = (job_df[\"submit_time\"] // 3600).astype(int)\n",
    "job_df[\"is_multi_worker\"] = job_df[\"worker_num\"] > 1\n",
    "job_df[\"duration_hours\"] = job_df[\"duration\"] / 3600.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9315f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing fleet...\n",
      "Total nodes 4278, total GPU cards 10412, GPU models 6\n"
     ]
    }
   ],
   "source": [
    "# Fleet summary\n",
    "print(\"Summarizing fleet...\")\n",
    "if not {\"gpu_capacity_num\",\"gpu_model\"}.issubset(node_df.columns):\n",
    "    raise ValueError(\"node_info_df.csv must include gpu_capacity_num and gpu_model\")\n",
    "\n",
    "fleet_cards = int(node_df[\"gpu_capacity_num\"].sum())\n",
    "fleet_nodes = len(node_df)\n",
    "fleet_models = node_df[\"gpu_model\"].nunique()\n",
    "\n",
    "by_model = (\n",
    "    node_df.groupby(\"gpu_model\")\n",
    "    .agg(nodes=(\"node_name\",\"count\"),\n",
    "         total_gpus=(\"gpu_capacity_num\",\"sum\"),\n",
    "         median_cpu=(\"cpu_num\",\"median\"),\n",
    "         max_gpus=(\"gpu_capacity_num\",\"max\"))\n",
    "    .sort_values(\"total_gpus\", ascending=False)\n",
    ")\n",
    "by_model.to_csv(TBLDIR / \"fleet_by_model.csv\")\n",
    "\n",
    "print(f\"Total nodes {fleet_nodes}, total GPU cards {fleet_cards}, GPU models {fleet_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e77e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing workload...\n"
     ]
    }
   ],
   "source": [
    "# Workload summary\n",
    "print(\"Summarizing workload...\")\n",
    "total_jobs = len(job_df)\n",
    "unique_orgs = job_df[\"organization\"].nunique() if \"organization\" in job_df.columns else np.nan\n",
    "coverage_days = float(job_df[\"submit_time\"].max()) / 86400.0\n",
    "\n",
    "by_type = (\n",
    "    job_df.groupby(\"job_type\", dropna=False)\n",
    "    .agg(jobs=(\"job_name\",\"count\"),\n",
    "         gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "         cpu_seconds=(\"cpu_seconds\",\"sum\"),\n",
    "         median_duration=(\"duration\",\"median\"),\n",
    "         p90_duration=(\"duration\", lambda s: float(np.quantile(s, 0.90))))\n",
    "    .sort_values(\"jobs\", ascending=False)\n",
    ")\n",
    "by_type.to_csv(TBLDIR / \"job_type_summary.csv\")\n",
    "\n",
    "medians = job_df[[\"cpu_request\",\"gpu_request\",\"worker_num\",\"duration\"]].median().rename(\"median\")\n",
    "medians.to_csv(TBLDIR / \"job_medians.csv\", header=True)\n",
    "\n",
    "multi_worker_share = float((job_df[\"worker_num\"] > 1).mean())\n",
    "max_workers = int(job_df[\"worker_num\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6876f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Totals\n",
    "total_gpu_seconds = float(job_df[\"gpu_seconds\"].sum())\n",
    "total_cpu_seconds = float(job_df[\"cpu_seconds\"].sum())\n",
    "gpu_years = total_gpu_seconds / (3600 * 24 * 365)\n",
    "cpu_years = total_cpu_seconds / (3600 * 24 * 365)\n",
    "\n",
    "# Demand by GPU model in jobs if available\n",
    "job_model_col = None\n",
    "for c in [\"gpu_model\",\"requested_gpu_model\",\"gpu_type\"]:\n",
    "    if c in job_df.columns:\n",
    "        job_model_col = c\n",
    "        break\n",
    "\n",
    "if job_model_col is not None:\n",
    "    demand_model = (\n",
    "        job_df.groupby(job_model_col)\n",
    "        .agg(jobs=(\"job_name\",\"count\"),\n",
    "             gpu_demand=(\"gpu_demand\",\"sum\"),\n",
    "             gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "             median_gpu_req=(\"gpu_request\",\"median\"))\n",
    "        .sort_values(\"jobs\", ascending=False)\n",
    "    )\n",
    "    demand_model[\"jobs_share\"] = demand_model[\"jobs\"] / demand_model[\"jobs\"].sum()\n",
    "    demand_model[\"gpu_seconds_share\"] = demand_model[\"gpu_seconds\"] / demand_model[\"gpu_seconds\"].sum()\n",
    "    demand_model.to_csv(TBLDIR / \"demand_by_model.csv\")\n",
    "else:\n",
    "    print(\"Job file has no GPU model column, skipping demand by model table.\")\n",
    "    demand_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "495170f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organization table\n",
    "if \"organization\" in job_df.columns:\n",
    "    org_tbl = (\n",
    "        job_df.groupby(\"organization\")\n",
    "        .agg(jobs=(\"job_name\",\"count\"),\n",
    "             gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "             gpu_demand=(\"gpu_demand\",\"sum\"),\n",
    "             median_duration=(\"duration\",\"median\"))\n",
    "        .sort_values(\"gpu_seconds\", ascending=False)\n",
    "    )\n",
    "    org_tbl.to_csv(TBLDIR / \"org_by_gpu_seconds.csv\")\n",
    "else:\n",
    "    org_tbl = None\n",
    "\n",
    "# Daily and hourly tables\n",
    "daily_tbl = (\n",
    "    job_df.groupby(\"submit_day\")\n",
    "    .agg(jobs=(\"job_name\",\"count\"),\n",
    "         hp_jobs=(\"job_type\", lambda s: int((s == \"HP\").sum()) if s.notna().any() else 0),\n",
    "         spot_jobs=(\"job_type\", lambda s: int((s == \"Spot\").sum()) if s.notna().any() else 0),\n",
    "         gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "         gpu_demand=(\"gpu_demand\",\"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "daily_tbl.to_csv(TBLDIR / \"daily_summary.csv\", index=False)\n",
    "\n",
    "hourly_tbl = (\n",
    "    job_df.groupby(\"submit_hour\")\n",
    "    .agg(jobs=(\"job_name\",\"count\"),\n",
    "         gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "         gpu_demand=(\"gpu_demand\",\"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "hourly_tbl.to_csv(TBLDIR / \"hourly_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e50e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles and long tail\n",
    "quantiles = job_df[\"duration\"].quantile([0.5, 0.9, 0.95, 0.99]).rename(\"duration_seconds\")\n",
    "quantiles.to_csv(TBLDIR / \"duration_quantiles.csv\", header=True)\n",
    "\n",
    "top_long = job_df.sort_values(\"duration\", ascending=False).head(200)\n",
    "cols_export = [c for c in [\"job_name\",\"organization\",\"job_type\",\"duration\",\"gpu_request\",\"cpu_request\",\"worker_num\",\"gpu_seconds\",\"submit_time\"] if c in job_df.columns]\n",
    "top_long[cols_export].to_csv(TBLDIR / \"longest_jobs_top200.csv\", index=False)\n",
    "\n",
    "top_gpu = job_df.sort_values(\"gpu_seconds\", ascending=False).head(200)\n",
    "top_gpu[cols_export].to_csv(TBLDIR / \"heaviest_gpu_jobs_top200.csv\", index=False)\n",
    "\n",
    "# Correlations\n",
    "corr_cols = [c for c in [\"cpu_request\",\"gpu_request\",\"worker_num\",\"duration\",\"gpu_demand\"] if c in job_df.columns]\n",
    "corr = job_df[corr_cols].corr(numeric_only=True)\n",
    "corr.to_csv(TBLDIR / \"correlations.csv\")\n",
    "\n",
    "# Worker gang distribution\n",
    "worker_dist = job_df[\"worker_num\"].value_counts().sort_index()\n",
    "worker_dist.to_csv(TBLDIR / \"worker_count_distribution.csv\", header=[\"jobs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8640bfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating figures...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figures\n",
    "print(\"Creating figures...\")\n",
    "\n",
    "# 1. Fleet GPUs by model\n",
    "plt.figure()\n",
    "node_df.groupby(\"gpu_model\")[\"gpu_capacity_num\"].sum().sort_values(ascending=False).plot(kind=\"bar\", title=\"Total GPUs by model\")\n",
    "plt.ylabel(\"GPU cards\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"fleet_gpus_by_model.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Job duration histogram, capped at seven days for readability\n",
    "plt.figure()\n",
    "job_df[\"duration\"].clip(upper=3600*24*7).plot(kind=\"hist\", bins=60, title=\"Job duration capped at seven days\")\n",
    "plt.xlabel(\"Seconds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"job_duration_hist.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3. Daily GPU seconds line\n",
    "plt.figure()\n",
    "daily_tbl.plot(x=\"submit_day\", y=\"gpu_seconds\", kind=\"line\", title=\"Daily GPU seconds\")\n",
    "plt.xlabel(\"Day index\")\n",
    "plt.ylabel(\"GPU seconds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"daily_gpu_seconds.png\")\n",
    "plt.close()\n",
    "\n",
    "# 4. CPU versus GPU request scatter, sample for speed\n",
    "plt.figure()\n",
    "sample_n = min(100000, len(job_df))\n",
    "job_df.sample(sample_n, random_state=42).plot(x=\"gpu_request\", y=\"cpu_request\", kind=\"scatter\", title=\"CPU request versus GPU request\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"cpu_vs_gpu_request.png\")\n",
    "plt.close()\n",
    "\n",
    "# 5. Worker count histogram\n",
    "plt.figure()\n",
    "job_df[\"worker_num\"].plot(kind=\"hist\", bins=60, title=\"Worker count distribution\")\n",
    "plt.xlabel(\"Worker count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"worker_count_hist.png\")\n",
    "plt.close()\n",
    "\n",
    "# 6. Organization top twenty by GPU seconds bar\n",
    "if org_tbl is not None:\n",
    "    plt.figure()\n",
    "    org_tbl.head(20)[\"gpu_seconds\"].plot(kind=\"bar\", title=\"Top orgs by GPU seconds\")\n",
    "    plt.ylabel(\"GPU seconds\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGDIR / \"org_top20_gpu_seconds.png\")\n",
    "    plt.close()\n",
    "\n",
    "# 7. GPU seconds by model from jobs if available\n",
    "if demand_model is not None:\n",
    "    plt.figure()\n",
    "    demand_model[\"gpu_seconds\"].sort_values(ascending=False).plot(kind=\"bar\", title=\"GPU seconds by model in jobs\")\n",
    "    plt.ylabel(\"GPU seconds\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGDIR / \"gpu_seconds_by_model_jobs.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa634807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Tables in outputs\\tables Figures in outputs\\figures Summary in outputs\\summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Summary text file\n",
    "with open(OUTDIR / \"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Dataset summary\\n\")\n",
    "    f.write(f\"Nodes {fleet_nodes}, GPU cards {fleet_cards}, GPU models {fleet_models}\\n\")\n",
    "    f.write(f\"Jobs {total_jobs}, Organizations {unique_orgs}\\n\")\n",
    "    f.write(f\"Trace span days {coverage_days:.2f}\\n\")\n",
    "    f.write(f\"Total GPU seconds {human(total_gpu_seconds)}, GPU years {gpu_years:.2f}\\n\")\n",
    "    f.write(f\"Total CPU seconds {human(total_cpu_seconds)}, CPU years {cpu_years:.2f}\\n\")\n",
    "    f.write(f\"Multi worker share {multi_worker_share:.2%}, Max workers {max_workers}\\n\")\n",
    "\n",
    "print(\"Done. Tables in\", TBLDIR, \"Figures in\", FIGDIR, \"Summary in\", OUTDIR / \"summary.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a0a30",
   "metadata": {},
   "source": [
    "# Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Enriching jobs...\n"
     ]
    }
   ],
   "source": [
    "# Extra dataset analysis that adds concurrency, capacity ratios, diurnal patterns, Pareto and Gini, buckets, heatmaps, and outlier finds.\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTDIR = Path(\"outputs_extra\")\n",
    "FIGDIR = OUTDIR / \"figures\"\n",
    "TBLDIR = OUTDIR / \"tables\"\n",
    "OUTDIR.mkdir(exist_ok=True, parents=True)\n",
    "FIGDIR.mkdir(exist_ok=True, parents=True)\n",
    "TBLDIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "# node_df = pd.read_csv(\"node_info_df.csv\")\n",
    "# job_df = pd.read_csv(\"job_info_df.csv\")\n",
    "\n",
    "node_df = pd.read_csv('/kaggle/input/alibaba-gpu-cluster-spot-resource-dataset/node_info_df.csv')\n",
    "job_df = pd.read_csv('/kaggle/input/alibaba-gpu-cluster-spot-resource-dataset/job_info_df.csv')\n",
    "\n",
    "print(\"Enriching jobs...\")\n",
    "job_df = job_df.copy()\n",
    "job_df[\"end_time\"] = job_df[\"submit_time\"] + job_df[\"duration\"]\n",
    "job_df[\"gpu_demand\"] = job_df[\"gpu_request\"] * job_df[\"worker_num\"]\n",
    "job_df[\"gpu_seconds\"] = job_df[\"gpu_demand\"] * job_df[\"duration\"]\n",
    "job_df[\"cpu_seconds\"] = job_df[\"cpu_request\"] * job_df[\"duration\"]\n",
    "job_df[\"hour_of_day\"] = ((job_df[\"submit_time\"] % 86400) // 3600).astype(int)\n",
    "job_df[\"submit_day\"] = (job_df[\"submit_time\"] // 86400).astype(int)\n",
    "job_df[\"is_multi_worker\"] = job_df[\"worker_num\"] > 1\n",
    "job_df[\"cpu_per_gpu\"] = job_df[\"cpu_request\"] / job_df[\"gpu_request\"].replace(0, np.nan)\n",
    "job_df[\"duration_hours\"] = job_df[\"duration\"] / 3600.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d95bb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hourly concurrency...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Capacity constants\n",
    "total_cards = int(node_df[\"gpu_capacity_num\"].sum())\n",
    "cap_gpu_seconds_per_day = total_cards * 86400\n",
    "\n",
    "# 1. Concurrency time series at hourly resolution for jobs and GPU demand\n",
    "print(\"Computing hourly concurrency...\")\n",
    "max_hour = int(np.ceil(job_df[\"end_time\"].max() / 3600.0))\n",
    "start_hour = (job_df[\"submit_time\"] // 3600).astype(int).values\n",
    "end_hour = ((job_df[\"end_time\"] - 1) // 3600).astype(int).values\n",
    "gpu_demand = job_df[\"gpu_demand\"].values\n",
    "ones = np.ones(len(job_df), dtype=float)\n",
    "\n",
    "def diff_to_series(starts, ends, weights, length):\n",
    "    diff = np.zeros(length + 2, dtype=float)\n",
    "    np.add.at(diff, starts, weights)\n",
    "    np.add.at(diff, ends + 1, -weights)\n",
    "    series = np.cumsum(diff)[: length + 1]\n",
    "    return series\n",
    "\n",
    "gpu_conc = diff_to_series(start_hour, end_hour, gpu_demand, max_hour)\n",
    "job_conc = diff_to_series(start_hour, end_hour, ones, max_hour)\n",
    "\n",
    "hour_idx = np.arange(len(gpu_conc))\n",
    "hourly = pd.DataFrame({\n",
    "    \"hour\": hour_idx,\n",
    "    \"jobs_active\": job_conc,\n",
    "    \"gpus_active_requested\": gpu_conc,\n",
    "    \"capacity_gpus\": total_cards,\n",
    "    \"demand_capacity_ratio\": gpu_conc / np.maximum(total_cards, 1)\n",
    "})\n",
    "hourly.to_csv(TBLDIR / \"hourly_concurrency.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "hourly.plot(x=\"hour\", y=\"gpus_active_requested\", kind=\"line\", title=\"Requested GPUs active per hour\")\n",
    "plt.xlabel(\"Hour since trace start\")\n",
    "plt.ylabel(\"GPUs requested active\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"hourly_gpu_concurrency.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "hourly.plot(x=\"hour\", y=\"demand_capacity_ratio\", kind=\"line\", title=\"Demand to capacity ratio per hour\")\n",
    "plt.xlabel(\"Hour since trace start\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"hourly_demand_capacity_ratio.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5575db66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing per model concurrency for top models...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Per model concurrency for top models by GPU seconds\n",
    "job_model_col = None\n",
    "for c in [\"gpu_model\",\"requested_gpu_model\",\"gpu_type\"]:\n",
    "    if c in job_df.columns:\n",
    "        job_model_col = c\n",
    "        break\n",
    "\n",
    "if job_model_col is not None:\n",
    "    print(\"Computing per model concurrency for top models...\")\n",
    "    top_models = job_df.groupby(job_model_col)[\"gpu_seconds\"].sum().sort_values(ascending=False).head(6).index.tolist()\n",
    "    model_conc = pd.DataFrame({\"hour\": hour_idx})\n",
    "    for m in top_models:\n",
    "        mask = job_df[job_model_col] == m\n",
    "        m_gpu = diff_to_series(start_hour[mask.values], end_hour[mask.values], gpu_demand[mask.values], max_hour)\n",
    "        model_conc[str(m)] = m_gpu\n",
    "        plt.figure()\n",
    "        pd.DataFrame({\"hour\": hour_idx, \"gpus\": m_gpu}).plot(x=\"hour\", y=\"gpus\", kind=\"line\", title=f\"Requested GPUs active per hour for {m}\")\n",
    "        plt.xlabel(\"Hour since trace start\")\n",
    "        plt.ylabel(\"GPUs requested active\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGDIR / f\"hourly_gpu_concurrency_{str(m)}.png\")\n",
    "        plt.close()\n",
    "    model_conc.to_csv(TBLDIR / \"hourly_concurrency_top_models.csv\", index=False)\n",
    "else:\n",
    "    print(\"No GPU model column found in jobs for per model concurrency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6b24c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing capacity pressure by day...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3. Capacity pressure by day, global and per model if available\n",
    "print(\"Computing capacity pressure by day...\")\n",
    "daily = (\n",
    "    job_df.groupby(\"submit_day\")\n",
    "    .agg(gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "         jobs=(\"gpu_seconds\",\"count\"))\n",
    "    .reset_index()\n",
    ")\n",
    "daily[\"capacity_gpu_seconds\"] = cap_gpu_seconds_per_day\n",
    "daily[\"demand_capacity_ratio\"] = daily[\"gpu_seconds\"] / np.maximum(daily[\"capacity_gpu_seconds\"], 1)\n",
    "daily.to_csv(TBLDIR / \"daily_capacity_pressure.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "daily.plot(x=\"submit_day\", y=\"demand_capacity_ratio\", kind=\"line\", title=\"Daily demand to capacity ratio\")\n",
    "plt.xlabel(\"Day since trace start\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"daily_demand_capacity_ratio.png\")\n",
    "plt.close()\n",
    "\n",
    "if job_model_col is not None:\n",
    "    cap_by_model = node_df.groupby(\"gpu_model\")[\"gpu_capacity_num\"].sum().rename(\"capacity_cards\")\n",
    "    demand_by_model_day = job_df.groupby([\"submit_day\", job_model_col])[\"gpu_seconds\"].sum().reset_index()\n",
    "    top_models = job_df.groupby(job_model_col)[\"gpu_seconds\"].sum().sort_values(ascending=False).head(6).index.tolist()\n",
    "    demand_top = demand_by_model_day[demand_by_model_day[job_model_col].isin(top_models)]\n",
    "    demand_piv = demand_top.pivot(index=\"submit_day\", columns=job_model_col, values=\"gpu_seconds\").fillna(0.0)\n",
    "    ratios = []\n",
    "    for m in demand_piv.columns:\n",
    "        cap_cards = float(cap_by_model.get(m, 0.0))\n",
    "        cap_sec = cap_cards * 86400.0\n",
    "        ratios.append(demand_piv[m] / max(cap_sec, 1.0))\n",
    "    ratio_df = pd.concat(ratios, axis=1)\n",
    "    ratio_df.columns = demand_piv.columns\n",
    "    ratio_df.to_csv(TBLDIR / \"daily_capacity_ratio_top_models.csv\")\n",
    "    plt.figure()\n",
    "    ratio_df.plot(kind=\"line\", title=\"Daily demand to capacity ratio by top models\")\n",
    "    plt.xlabel(\"Day since trace start\")\n",
    "    plt.ylabel(\"Ratio\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGDIR / \"daily_demand_capacity_ratio_by_model.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae49a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing diurnal patterns...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Diurnal patterns by hour of day\n",
    "print(\"Computing diurnal patterns...\")\n",
    "hod = (\n",
    "    job_df.groupby(\"hour_of_day\")\n",
    "    .agg(jobs=(\"gpu_seconds\",\"count\"),\n",
    "         gpu_seconds=(\"gpu_seconds\",\"sum\"),\n",
    "         median_duration=(\"duration\",\"median\"))\n",
    "    .reset_index()\n",
    ")\n",
    "hod.to_csv(TBLDIR / \"hour_of_day_patterns.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "hod.plot(x=\"hour_of_day\", y=\"jobs\", kind=\"bar\", title=\"Jobs per hour of day\")\n",
    "plt.xlabel(\"Hour of day\")\n",
    "plt.ylabel(\"Jobs\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"jobs_per_hour_of_day.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "hod.plot(x=\"hour_of_day\", y=\"gpu_seconds\", kind=\"bar\", title=\"GPU seconds per hour of day\")\n",
    "plt.xlabel(\"Hour of day\")\n",
    "plt.ylabel(\"GPU seconds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"gpu_seconds_per_hour_of_day.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca46c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Pareto and Gini across organizations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulta\\AppData\\Local\\Temp\\ipykernel_33392\\554595160.py:17: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return 1.0 - 2.0 * np.trapz(rel, dx=1.0) / n\n"
     ]
    }
   ],
   "source": [
    "# 5. Pareto and Gini across organizations\n",
    "if \"organization\" in job_df.columns:\n",
    "    print(\"Computing Pareto and Gini across organizations...\")\n",
    "    org_demand = job_df.groupby(\"organization\")[\"gpu_seconds\"].sum().sort_values(ascending=False)\n",
    "    org_share = org_demand / max(org_demand.sum(), 1.0)\n",
    "    cum_share = org_share.cumsum()\n",
    "    pareto = pd.DataFrame({\"organization\": org_share.index, \"gpu_seconds\": org_demand.values, \"share\": org_share.values, \"cum_share\": cum_share.values})\n",
    "    pareto.to_csv(TBLDIR / \"org_pareto_gpu_seconds.csv\", index=False)\n",
    "\n",
    "    def gini(x):\n",
    "        x = np.sort(np.asarray(x, dtype=float))\n",
    "        if x.size == 0:\n",
    "            return np.nan\n",
    "        cum = np.cumsum(x)\n",
    "        rel = cum / cum[-1] if cum[-1] > 0 else cum\n",
    "        n = x.size\n",
    "        return 1.0 - 2.0 * np.trapz(rel, dx=1.0) / n\n",
    "\n",
    "    gini_val = gini(org_demand.values)\n",
    "    with open(OUTDIR / \"summary_extra.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Gini across organizations by GPU seconds {gini_val:.4f}\\n\")\n",
    "\n",
    "    plt.figure()\n",
    "    x = np.linspace(0, 1, len(org_share), endpoint=True)\n",
    "    y = cum_share.values\n",
    "    plt.plot(x, y)\n",
    "    plt.plot([0,1], [0,1])\n",
    "    plt.title(\"Lorenz curve for GPU seconds across organizations\")\n",
    "    plt.xlabel(\"Cumulative organizations\")\n",
    "    plt.ylabel(\"Cumulative GPU seconds share\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGDIR / \"lorenz_curve_orgs.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10cfd7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bucketed summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulta\\AppData\\Local\\Temp\\ipykernel_33392\\796056133.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  job_df.groupby(col)\n",
      "C:\\Users\\sulta\\AppData\\Local\\Temp\\ipykernel_33392\\796056133.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  job_df.groupby(col)\n",
      "C:\\Users\\sulta\\AppData\\Local\\Temp\\ipykernel_33392\\796056133.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  job_df.groupby(col)\n",
      "C:\\Users\\sulta\\AppData\\Local\\Temp\\ipykernel_33392\\796056133.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  job_df.groupby(col)\n"
     ]
    }
   ],
   "source": [
    "# 6. Buckets for size classes\n",
    "print(\"Computing bucketed summaries...\")\n",
    "dur_bins = [0, 60, 600, 3600, 21600, 86400, 259200, np.inf]\n",
    "dur_labels = [\"0 to 1 min\",\"1 to 10 min\",\"10 to 60 min\",\"1 to 6 hours\",\"6 to 24 hours\",\"1 to 3 days\",\"over 3 days\"]\n",
    "job_df[\"duration_bucket\"] = pd.cut(job_df[\"duration\"], bins=dur_bins, labels=dur_labels, right=False)\n",
    "\n",
    "gpu_bins = [0,1,2,5,9,17,33,65,129, np.inf]\n",
    "gpu_labels = [\"0\",\"1\",\"2 to 4\",\"5 to 8\",\"9 to 16\",\"17 to 32\",\"33 to 64\",\"65 to 128\",\"129 plus\"]\n",
    "job_df[\"gpu_demand_bucket\"] = pd.cut(job_df[\"gpu_demand\"], bins=gpu_bins, labels=gpu_labels, right=False, include_lowest=True)\n",
    "\n",
    "wrk_bins = [1,2,9,33,129, np.inf]\n",
    "wrk_labels = [\"1\",\"2 to 8\",\"9 to 32\",\"33 to 128\",\"129 plus\"]\n",
    "job_df[\"worker_bucket\"] = pd.cut(job_df[\"worker_num\"], bins=wrk_bins, labels=wrk_labels, right=False, include_lowest=True)\n",
    "\n",
    "ratio_bins = [0,1,2,4,8,16,32,64,128, np.inf]\n",
    "ratio_labels = [\"0 to 1\",\"1 to 2\",\"2 to 4\",\"4 to 8\",\"8 to 16\",\"16 to 32\",\"32 to 64\",\"64 to 128\",\"over 128\"]\n",
    "job_df[\"cpu_per_gpu_bucket\"] = pd.cut(job_df[\"cpu_per_gpu\"], bins=ratio_bins, labels=ratio_labels, right=False)\n",
    "\n",
    "def bucket_table(col):\n",
    "    tbl = (\n",
    "        job_df.groupby(col)\n",
    "        .agg(jobs=(\"gpu_seconds\",\"count\"),\n",
    "             share_jobs=(\"gpu_seconds\", lambda s: len(s)),\n",
    "             gpu_seconds=(\"gpu_seconds\",\"sum\"))\n",
    "        .sort_values(\"jobs\", ascending=False)\n",
    "    )\n",
    "    total_jobs = tbl[\"jobs\"].sum()\n",
    "    total_gpu = tbl[\"gpu_seconds\"].sum()\n",
    "    tbl[\"jobs_share\"] = tbl[\"jobs\"] / max(total_jobs, 1)\n",
    "    tbl[\"gpu_seconds_share\"] = tbl[\"gpu_seconds\"] / max(total_gpu, 1)\n",
    "    return tbl\n",
    "\n",
    "bucket_table(\"duration_bucket\").to_csv(TBLDIR / \"bucket_duration.csv\")\n",
    "bucket_table(\"gpu_demand_bucket\").to_csv(TBLDIR / \"bucket_gpu_demand.csv\")\n",
    "bucket_table(\"worker_bucket\").to_csv(TBLDIR / \"bucket_worker_num.csv\")\n",
    "bucket_table(\"cpu_per_gpu_bucket\").to_csv(TBLDIR / \"bucket_cpu_per_gpu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dca7ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating heatmaps...\n"
     ]
    }
   ],
   "source": [
    "# 7. Heatmaps for bivariate structure\n",
    "print(\"Creating heatmaps...\")\n",
    "plt.figure()\n",
    "plt.hist2d(job_df[\"gpu_request\"], job_df[\"duration_hours\"], bins=[50,50])\n",
    "plt.xlabel(\"GPU request per worker\")\n",
    "plt.ylabel(\"Duration hours\")\n",
    "plt.title(\"GPU request versus duration heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"heatmap_gpu_request_vs_duration.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist2d(job_df[\"worker_num\"], job_df[\"duration_hours\"], bins=[50,50])\n",
    "plt.xlabel(\"Worker count\")\n",
    "plt.ylabel(\"Duration hours\")\n",
    "plt.title(\"Worker count versus duration heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"heatmap_workers_vs_duration.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89a158c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing inter arrival and burstiness...\n"
     ]
    }
   ],
   "source": [
    "# 8. Inter arrival times and burstiness\n",
    "print(\"Computing inter arrival and burstiness...\")\n",
    "times = np.sort(job_df[\"submit_time\"].values)\n",
    "inter_arrival = np.diff(times)\n",
    "inter_q = np.quantile(inter_arrival, [0.5,0.9,0.95,0.99]) if inter_arrival.size > 0 else np.array([np.nan]*4)\n",
    "pd.DataFrame({\"quantile\":[0.5,0.9,0.95,0.99], \"seconds\":inter_q}).to_csv(TBLDIR / \"inter_arrival_quantiles.csv\", index=False)\n",
    "\n",
    "counts_per_hour = np.bincount(start_hour, minlength=max_hour+1).astype(float)\n",
    "mean_c = counts_per_hour.mean() if counts_per_hour.size > 0 else np.nan\n",
    "var_c = counts_per_hour.var() if counts_per_hour.size > 0 else np.nan\n",
    "fano = var_c / mean_c if mean_c and mean_c > 0 else np.nan\n",
    "cv = np.sqrt(var_c) / mean_c if mean_c and mean_c > 0 else np.nan\n",
    "with open(OUTDIR / \"summary_extra.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Inter arrival median seconds {float(inter_q[0]) if inter_arrival.size>0 else float('nan'):.2f}\\n\")\n",
    "    f.write(f\"Hourly arrival mean {mean_c:.2f} variance {var_c:.2f} fano {fano:.3f} cv {cv:.3f}\\n\")\n",
    "\n",
    "plt.figure()\n",
    "pd.Series(inter_arrival).clip(upper=np.quantile(inter_arrival, 0.99) if inter_arrival.size>0 else 1).plot(kind=\"hist\", bins=60, title=\"Inter arrival seconds clipped at p99\")\n",
    "plt.xlabel(\"Seconds\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGDIR / \"inter_arrival_hist.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ce20f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Spot and HP slices...\n"
     ]
    }
   ],
   "source": [
    "# 9. Spot and HP risk slices if job_type exists\n",
    "if \"job_type\" in job_df.columns:\n",
    "    print(\"Computing Spot and HP slices...\")\n",
    "    tp = job_df[\"job_type\"].fillna(\"Unknown\")\n",
    "    cuts = pd.DataFrame({\n",
    "        \"type\": [\"HP\",\"Spot\"],\n",
    "        \"jobs_share\": [float((tp==\"HP\").mean()), float((tp==\"Spot\").mean())],\n",
    "        \"gpu_seconds_share\": [\n",
    "            float(job_df.loc[tp==\"HP\",\"gpu_seconds\"].sum() / max(job_df[\"gpu_seconds\"].sum(),1.0)),\n",
    "            float(job_df.loc[tp==\"Spot\",\"gpu_seconds\"].sum() / max(job_df[\"gpu_seconds\"].sum(),1.0)),\n",
    "        ],\n",
    "    })\n",
    "    cuts.to_csv(TBLDIR / \"job_type_shares.csv\", index=False)\n",
    "\n",
    "    long_thr = 6*3600\n",
    "    long_tbl = job_df.assign(is_long = job_df[\"duration\"] >= long_thr).groupby([\"is_long\", \"job_type\"]).agg(\n",
    "        jobs=(\"gpu_seconds\",\"count\"),\n",
    "        gpu_seconds=(\"gpu_seconds\",\"sum\")\n",
    "    ).reset_index()\n",
    "    long_tbl.to_csv(TBLDIR / \"long_job_slices_by_type.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ca5c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing model mix per organization...\n",
      "Finding outliers...\n",
      "Extra analysis complete. Tables in outputs_extra\\tables Figures in outputs_extra\\figures Extra summary in outputs_extra\\summary_extra.txt\n"
     ]
    }
   ],
   "source": [
    "# 10. Model mix per organization if columns exist\n",
    "if job_model_col is not None and \"organization\" in job_df.columns:\n",
    "    print(\"Computing model mix per organization...\")\n",
    "    org_model = job_df.pivot_table(index=\"organization\", columns=job_model_col, values=\"gpu_seconds\", aggfunc=\"sum\").fillna(0.0)\n",
    "    org_model_share = org_model.div(org_model.sum(axis=1).replace(0.0, np.nan), axis=0)\n",
    "    org_model_share.to_csv(TBLDIR / \"org_model_mix_share.csv\")\n",
    "\n",
    "# 11. Outlier finds\n",
    "print(\"Finding outliers...\")\n",
    "short_heavy = job_df[(job_df[\"duration\"] <= 300) & (job_df[\"gpu_demand\"] >= 32)].copy()\n",
    "keep_cols = [c for c in [\"job_name\",\"organization\",\"job_type\",\"duration\",\"gpu_request\",\"worker_num\",\"gpu_demand\",\"cpu_request\",\"submit_time\"] if c in job_df.columns]\n",
    "short_heavy[keep_cols].sort_values(\"gpu_demand\", ascending=False).head(500).to_csv(TBLDIR / \"outliers_short_heavy.csv\", index=False)\n",
    "\n",
    "high_ratio = job_df[job_df[\"cpu_per_gpu\"] >= 64].copy()\n",
    "high_ratio[keep_cols + [\"cpu_per_gpu\"]].sort_values(\"cpu_per_gpu\", ascending=False).head(500).to_csv(TBLDIR / \"outliers_high_cpu_per_gpu.csv\", index=False)\n",
    "\n",
    "print(\"Extra analysis complete. Tables in\", TBLDIR, \"Figures in\", FIGDIR, \"Extra summary in\", OUTDIR / \"summary_extra.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
